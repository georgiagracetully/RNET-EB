{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['RNETEB_PATH'] = '/home/users/gtully/RNET-EB'\n",
    "os.environ['RANGER_PATH'] = '/home/users/gtully/Ranger-Deep-Learning-Optimizer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T18:55:02.070541Z",
     "iopub.status.busy": "2024-08-28T18:55:02.070168Z",
     "iopub.status.idle": "2024-08-28T18:55:06.242055Z",
     "shell.execute_reply": "2024-08-28T18:55:06.240995Z",
     "shell.execute_reply.started": "2024-08-28T18:55:02.070512Z"
    },
    "id": "8FNdkyw5AJxT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import ast\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ast import literal_eval\n",
    "import sklearn\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "sys.path.append(os.environ['RNETEB_PATH']+ '/tools')\n",
    "sys.path.append(os.environ['RANGER_PATH'] + '/ranger')\n",
    "from plotting import *   # from RNETEB tools \n",
    "from training import *   # from RNETEB tools\n",
    "\n",
    "from ranger import Ranger # from Ranger-Deep-Learning-Optimizer cloned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNET_EB_000_NPT Training Important Notes\n",
    "\n",
    "## NPT means \"No Pre-Training\"\n",
    "\n",
    "## THIS NOTEBOOK EVALUATES RNET_000 WITHOUT PRETRAINED RNET WEIGHTS! \n",
    "\n",
    "1) This model was trained on both the logkd_lig_scaled and the logkd_nolig_scaled, per riboswitch sequence (see the uniquely defined RNA_Dataset in this notebook). \n",
    "\n",
    "2) Hyperparameters were chosen from success of fine-tuning RibonanzaNet on secondary structure data. \n",
    "    - optimizer: Ranger\n",
    "    - scheduler: CosineAnnealingLR\n",
    "    - criterion: L1\n",
    "\n",
    "3) Before running all cells in the NB, make sure to clone both RNET-EB and the Ranger-Deep-Learning-Optimizer Repository and add paths to : \n",
    "    os.environ['RNETEB_PATH'] = '/your/path/to/RNET-EB'\n",
    "    os.environ['RANGER_PATH'] = '/your/path/to/Ranger-Deep-Learning-Optimizer/'\n",
    "\n",
    "4) As is, this repository regenerates data in locations where I already have data saved in public RNET-EB repository. If you would like to save results separately, rename the output files. \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Cleaned Data from RibonanzaNet_EB_data_prep.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(os.environ['RNETEB_PATH'] + '/data/processed_data/RNET_EB_train.json')\n",
    "val_df = pd.read_json(os.environ['RNETEB_PATH'] + '/data/processed_data/RNET_EB_val.json')\n",
    "test_df = pd.read_json(os.environ['RNETEB_PATH'] + '/data/processed_data/RNET_EB_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Activation Ratio', 'Design', 'Folding_Subscore', 'KDFMN', 'KDOFF',\n",
       "       'KDON', 'KDnoFMN', 'Kd_OFF', 'Kd_ON', 'NumberOfClusters', 'Player',\n",
       "       'Puzzle_Name', 'Round', 'index', 'ligand', 'min_kd_val', 'puzzle',\n",
       "       'sequence', 'switch', 'MS2_aptamer', 'lig_aptamer', 'MS2_lig_aptamer',\n",
       "       'constraints_worked', 'logkd_nolig', 'logkd_lig', 'logkd_nolig_scaled',\n",
       "       'logkd_lig_scaled', 'Dataset', 'passed_CDHIT_filter', 'log_AR', 'id',\n",
       "       'description'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFBa8LevAJxW"
   },
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T18:55:09.660522Z",
     "iopub.status.busy": "2024-08-28T18:55:09.660017Z",
     "iopub.status.idle": "2024-08-28T18:55:09.668015Z",
     "shell.execute_reply": "2024-08-28T18:55:09.666707Z",
     "shell.execute_reply.started": "2024-08-28T18:55:09.660493Z"
    },
    "id": "O0_rpS0oAJxY"
   },
   "outputs": [],
   "source": [
    "class RNA_Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.tokens = {nt: i for i, nt in enumerate('ACGU')}\n",
    "        self.label_names = ['logkd_lig_scaled', 'logkd_nolig_scaled']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence=[self.tokens[nt] for nt in (self.data.loc[idx,'sequence'])]\n",
    "        sequence=np.array(sequence)\n",
    "        sequence=torch.tensor(sequence)\n",
    "\n",
    "        labels = np.array([self.data.loc[idx, l] for l in self.label_names])  # Just 1 value per label\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)  # Ensure labels are of correct float type\n",
    "\n",
    "\n",
    "        return {'sequence': sequence, 'labels': labels}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1EA_tT5gv_I1",
    "outputId": "9f374ac7-843f-4471-bc76-17724fc64301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Create the datasets\n",
    "train_dataset = RNA_Dataset(train_df)\n",
    "val_dataset = RNA_Dataset(val_df)\n",
    "\n",
    "# Test the first sample\n",
    "print(train_dataset[0]['labels'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NSkiyrwVc4ui"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LwZkeoo5RfFe"
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.environ['RNETEB_PATH']+'/ribonanzanet2d-final')\n",
    "\n",
    "from Network import *\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "        self.entries=entries\n",
    "\n",
    "    def print(self):\n",
    "        print(self.entries)\n",
    "\n",
    "def load_config_from_yaml(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return Config(**config)\n",
    "\n",
    "class finetuned_RibonanzaNet(RibonanzaNet):\n",
    "    def __init__(self, config, pretrained=False):\n",
    "        super(finetuned_RibonanzaNet, self).__init__(config)\n",
    "        if pretrained:\n",
    "            self.load_state_dict(torch.load(os.environ['RNETEB_PATH']+'/ribonanzanet-weights/RibonanzaNet.pt',map_location='cpu'))\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.decoder = nn.Linear(64, 2)  # From 64 \"pooled values from each channel \" to 2 output labels\n",
    "\n",
    "    def forward(self,src):\n",
    "\n",
    "        sequence_features, pairwise_features=self.get_embeddings(src, torch.ones_like(src).long().to(src.device))\n",
    "        pairwise_features = pairwise_features.squeeze(0)  # Remove the batch dimension to make it [H, W, 64]\n",
    "        pairwise_features = pairwise_features.permute(2, 0, 1)  # Change to [64, H, W] to match pooling expectation (C, H, W)\n",
    "\n",
    "        # Apply global average pooling, result is [64, 1, 1]\n",
    "        pairwise_features = self.global_pool(pairwise_features)\n",
    "\n",
    "        # Flatten the output to [64]\n",
    "        pairwise_features = pairwise_features.view(pairwise_features.size(0))  # Flatten to [64] (batch size 1, so this will be [64])\n",
    "\n",
    "        # Pass through the decoder to get the final output [2]\n",
    "        output = self.decoder(pairwise_features)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the most important cell for this NB - pretrained = False ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iTkTembdN2S",
    "outputId": "863b6eca-a0e4-42bc-b47f-5c0397207ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing 9 ConvTransformerEncoderLayers\n"
     ]
    }
   ],
   "source": [
    "config=load_config_from_yaml(os.environ['RNETEB_PATH'] + \"/ribonanzanet2d-final/configs/pairwise.yaml\")\n",
    "model=finetuned_RibonanzaNet(config,pretrained=False).cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZI-tp8gdPn_"
   },
   "source": [
    "## Training Loop\n",
    "\n",
    "Note: Need to clone the Ranger-Deep-Learning-Optimizer and add the path to the ranger folder within the repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the epoch losses as well as weights and model check point. \n",
    "### Also save plots of training and validation losses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n"
     ]
    }
   ],
   "source": [
    "# Initialize Hyperparameters\n",
    "epochs = 20\n",
    "cos_epoch = 15\n",
    "\n",
    "best_loss = np.inf\n",
    "optimizer = Ranger(model.parameters(), weight_decay=0.001, lr=0.0001)\n",
    "criterion = torch.nn.L1Loss()\n",
    "schedule = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(epochs - cos_epoch) * len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/users/gtully/Ranger-Deep-Learning-Optimizer//ranger/ranger.py:138: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value = 1) (Triggered internally at /pytorch/torch/csrc/utils/python_arg_parser.cpp:1661.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "Epoch 1 Loss: 0.9246442714300368: 100%|██████████| 1258/1258 [03:10<00:00,  6.59it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.9246, Val Loss: 0.8676\n",
      "✓ New best model saved! Val Loss: 0.8676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 2 Loss: 0.8835163648537877: 100%|██████████| 1258/1258 [03:11<00:00,  6.55it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 0.8835, Val Loss: 0.8655\n",
      "✓ New best model saved! Val Loss: 0.8655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 3 Loss: 0.8760259621710011: 100%|██████████| 1258/1258 [03:11<00:00,  6.57it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 0.8760, Val Loss: 0.8839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 4 Loss: 0.8702589942465146: 100%|██████████| 1258/1258 [03:11<00:00,  6.57it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 0.8703, Val Loss: 0.8881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 5 Loss: 0.8677642393282753: 100%|██████████| 1258/1258 [03:11<00:00,  6.58it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 0.8678, Val Loss: 0.9222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 7 Loss: 0.8542737276404385: 100%|██████████| 1258/1258 [03:11<00:00,  6.57it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 0.8543, Val Loss: 1.0413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 8 Loss: 0.8347998569919685:  34%|███▍      | 427/1258 [01:05<02:02,  6.78it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 12 Loss: 0.8183890864703158: 100%|██████████| 1258/1258 [03:12<00:00,  6.54it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Train Loss: 0.8184, Val Loss: 0.8745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 13 Loss: 0.8039874791956858: 100%|██████████| 1258/1258 [03:12<00:00,  6.54it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Train Loss: 0.8040, Val Loss: 0.8494\n",
      "✓ New best model saved! Val Loss: 0.8494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 14 Loss: 0.7917601759227167: 100%|██████████| 1258/1258 [03:11<00:00,  6.55it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Train Loss: 0.7918, Val Loss: 0.8382\n",
      "✓ New best model saved! Val Loss: 0.8382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 15 Loss: 0.7753875519967799: 100%|██████████| 1258/1258 [03:11<00:00,  6.56it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Train Loss: 0.7754, Val Loss: 0.8429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 16 Loss: 0.7608440926088248: 100%|██████████| 1258/1258 [03:12<00:00,  6.55it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Train Loss: 0.7608, Val Loss: 0.8416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 17 Loss: 0.7317241672159946: 100%|██████████| 1258/1258 [03:12<00:00,  6.55it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Train Loss: 0.7317, Val Loss: 0.9137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 18 Loss: 0.6938038559805229: 100%|██████████| 1258/1258 [03:11<00:00,  6.58it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Train Loss: 0.6938, Val Loss: 0.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 19 Loss: 0.6498583423030017: 100%|██████████| 1258/1258 [03:11<00:00,  6.57it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Train Loss: 0.6499, Val Loss: 0.8788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 20 Loss: 0.6136457161986771: 100%|██████████| 1258/1258 [03:11<00:00,  6.57it/s]\n",
      "  0%|          | 0/629 [00:00<?, ?it/s]/home/users/gtully/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 629/629 [00:17<00:00, 35.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Train Loss: 0.6136, Val Loss: 0.8853\n",
      "\n",
      "==================================================\n",
      "Training Complete!\n",
      "Best Validation Loss: 0.8382\n",
      "Checkpoints saved to: /home/users/gtully/RNET-EB/results/checkpoints\n",
      "Loss curves saved to: /home/users/gtully/RNET-EB/results/figures\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup directories\n",
    "checkpoint_dir = os.path.join(os.environ['RNETEB_PATH'], 'results/checkpoints')\n",
    "figure_dir = os.path.join(os.environ['RNETEB_PATH'], 'results/figures')\n",
    "Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Track losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    tbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "    oom = 0\n",
    "    for idx, batch in enumerate(tbar):\n",
    "        sequence = batch['sequence'].cuda()\n",
    "        labels = batch['labels'].cuda()\n",
    "        output = model(sequence) \n",
    "        labels = labels.view_as(output)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, labels)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (epoch + 1) > cos_epoch:\n",
    "            schedule.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        tbar.set_description(f\"Epoch {epoch + 1} Loss: {total_loss / (idx + 1)}\")\n",
    "\n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    tbar = tqdm(val_loader)\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_loss = 0\n",
    "    for idx, batch in enumerate(tbar):\n",
    "        sequence = batch['sequence'].cuda()\n",
    "        labels = batch['labels'].cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(sequence)\n",
    "            labels = labels.view_as(output)\n",
    "            #output = output.squeeze() ## Double check this \n",
    "            loss = criterion(output, labels)\n",
    "            loss = loss.mean()\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        val_preds.append([labels.cpu().numpy(), output.cpu().numpy()])\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch + 1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save latest checkpoint\n",
    "    scheduler_to_save = schedule if (epoch + 1) > cos_epoch else None\n",
    "    save_checkpoint(epoch, model, optimizer, scheduler_to_save, \n",
    "                   avg_train_loss, val_loss, train_losses, val_losses, \n",
    "                   best_loss, checkpoint_dir, 'latest_checkpoint.pt')\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_preds = val_preds\n",
    "        \n",
    "        # Save best weights \n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(os.environ['RNETEB_PATH'], \n",
    "                               'results/rnet_eb_weights/RibonanzaNet-EB_000_NPT_log_kds.pt'))\n",
    "        \n",
    "        # Save best checkpoint (full)\n",
    "        save_checkpoint(epoch, model, optimizer, scheduler_to_save, \n",
    "                       avg_train_loss, val_loss, train_losses, val_losses, \n",
    "                       best_loss, checkpoint_dir, 'RNET_EB_000_NPT_best_checkpoint.pt')\n",
    "        print(f\"✓ New best model saved! Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save periodic checkpoints every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        save_checkpoint(epoch, model, optimizer, scheduler_to_save, \n",
    "                       avg_train_loss, val_loss, train_losses, val_losses, \n",
    "                       best_loss, checkpoint_dir, f'RNET_EB_000_NPT_checkpoint_epoch_{epoch+1}.pt')\n",
    "\n",
    "    # Plot and save loss curves\n",
    "    plot_loss_curve(train_losses, val_losses, figure_dir)\n",
    "\n",
    "# Plot final summary\n",
    "plot_final_summary(train_losses, val_losses, figure_dir)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"Best Validation Loss: {best_loss:.4f}\")\n",
    "print(f\"Checkpoints saved to: {checkpoint_dir}\")\n",
    "print(f\"Loss curves saved to: {figure_dir}\")\n",
    "print(f\"{'='*50}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4278124,
     "sourceId": 7364321,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4278078,
     "sourceId": 7633917,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4299272,
     "sourceId": 7639698,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4299302,
     "sourceId": 7649407,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4459124,
     "sourceId": 8318191,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5571224,
     "sourceId": 9213562,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5598993,
     "sourceId": 9254397,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
